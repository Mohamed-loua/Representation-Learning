Experiment,Experiment Description,Total train time,Avg eval time,Avg. eval acc,Avg. eval f1,Avg train loss,Avg. eval loss
1,"LSTM, no dropout, encoder only",312.52020239830017,0.13355422019958496,56.37500000000001,0.41817499999999996,0.6839777869759792,0.68325986713171
2,"LSTM, dropout, encoder only",538.3260719776154,0.2103332281112671,56.47500000000001,0.57575,0.6864773277635985,0.6842519715428352
3,"LSTM, dropout, encoder-decoder, no attention",394.20300936698914,0.15619319677352905,53.224999999999994,0.6249750000000001,0.6880596487815245,0.687251016497612
4,"LSTM, dropout, encoder-decoder, with attention",441.81335973739624,0.1815739870071411,53.3,0.62535,0.6880390315675867,0.6870479732751846
5,"Transformer, 2 layers, pre-normalization",443.8185110092163,0.1788785457611084,56.325,0.538675,0.6965529602667967,0.6858257055282593
6,"Transformer, 4 layers, pre-normalization",1049.7526161670685,0.44528865814208984,58.175000000000004,0.55655,0.6876739523828138,0.6709730103611946
7,"Transformer, 2 layers, post-normalization",445.8235855102539,0.17854911088943481,54.25,0.540575,0.7039229143444973,0.691271223127842
8,Fine-tuning BERT,18141.46143746376,7.240021049976349,86.65,0.863175,0.4617029693474827,0.40727124735713005
